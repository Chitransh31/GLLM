{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf155134",
   "metadata": {},
   "source": [
    "# Hugging Face Authentication Troubleshooting\n",
    "\n",
    "This notebook helps you troubleshoot authentication issues when accessing gated Hugging Face repositories like `bigcode/starcoderbase-3b`. It provides step-by-step guidance to:\n",
    "\n",
    "1. **Check token validity** - Verify your Hugging Face token is working\n",
    "2. **Handle access errors** - Properly catch and handle gated repository errors \n",
    "3. **Request access** - Get instructions for requesting access to gated models\n",
    "4. **Implement fallbacks** - Automatically use public models when gated models aren't accessible\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "The error occurs when trying to access `bigcode/starcoderbase-3b`, which is a **gated repository** that requires:\n",
    "- A valid Hugging Face account\n",
    "- An authenticated token\n",
    "- Approval to access the specific model\n",
    "\n",
    "Let's walk through the solution step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import toml\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login, HfApi, hf_hub_download\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"‚úÖ All required libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97943b8",
   "metadata": {},
   "source": [
    "## Step 1: Load Secrets and Authenticate\n",
    "\n",
    "First, let's load your Hugging Face token from the secrets file and authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets from the .streamlit/secrets.toml file\n",
    "try:\n",
    "    secrets_file_path = os.path.abspath(os.path.join('..', '.streamlit', 'secrets.toml'))\n",
    "    secrets = toml.load(secrets_file_path)\n",
    "    hf_token = secrets.get(\"huggingface_token\")\n",
    "    \n",
    "    if hf_token and hf_token.strip():\n",
    "        print(\"‚úÖ Hugging Face token found in secrets.toml\")\n",
    "        print(f\"Token length: {len(hf_token)} characters\")\n",
    "        print(f\"Token preview: {hf_token[:8]}...{hf_token[-8:] if len(hf_token) > 16 else ''}\")\n",
    "    else:\n",
    "        print(\"‚ùå No Hugging Face token found or token is empty\")\n",
    "        print(\"Please add your token to .streamlit/secrets.toml\")\n",
    "        hf_token = None\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå secrets.toml file not found\")\n",
    "    print(\"Please create .streamlit/secrets.toml with your huggingface_token\")\n",
    "    hf_token = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading secrets: {e}\")\n",
    "    hf_token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test authentication with Hugging Face\n",
    "if hf_token:\n",
    "    try:\n",
    "        # Login to Hugging Face\n",
    "        login(hf_token, add_to_git_credential=True)\n",
    "        print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "        \n",
    "        # Test API access\n",
    "        api = HfApi()\n",
    "        user_info = api.whoami(token=hf_token)\n",
    "        print(f\"‚úÖ Authenticated as: {user_info['name']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Authentication failed: {e}\")\n",
    "        print(\"Please check your token is valid:\")\n",
    "        print(\"1. Go to https://huggingface.co/settings/tokens\")\n",
    "        print(\"2. Generate a new token if needed\")\n",
    "        print(\"3. Update your .streamlit/secrets.toml file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping authentication - no token available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4b190",
   "metadata": {},
   "source": [
    "## Step 2: Test Access to Gated Repository\n",
    "\n",
    "Now let's test access to the gated `bigcode/starcoderbase-3b` model and handle the OSError properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test access to the gated StarCoder model\n",
    "gated_model_name = \"bigcode/starcoderbase-3b\"\n",
    "access_granted = False\n",
    "\n",
    "print(f\"üîç Testing access to gated model: {gated_model_name}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Try to access the model configuration (lightweight test)\n",
    "    print(\"Attempting to access model config...\")\n",
    "    \n",
    "    # This is the line that was causing the original error\n",
    "    config = AutoModelForCausalLM.from_pretrained(\n",
    "        gated_model_name, \n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ SUCCESS: Access granted to gated repository!\")\n",
    "    print(\"You can now use the Fine-tuned StarCoder model.\")\n",
    "    access_granted = True\n",
    "    \n",
    "except OSError as e:\n",
    "    if \"403 Client Error\" in str(e) or \"gated repo\" in str(e):\n",
    "        print(\"‚ùå ACCESS DENIED: You don't have access to this gated repository\")\n",
    "        print(\"\\nüîß To request access:\")\n",
    "        print(\"1. Visit: https://huggingface.co/bigcode/starcoderbase-3b\")\n",
    "        print(\"2. Click 'Request Access' button\")\n",
    "        print(\"3. Fill out the form explaining your use case\")\n",
    "        print(\"4. Wait for approval (usually takes 1-2 business days)\")\n",
    "        print(\"\\nüí° In the meantime, we'll use a public model as fallback\")\n",
    "        \n",
    "    elif \"token\" in str(e).lower():\n",
    "        print(\"‚ùå TOKEN ERROR: Invalid or missing authentication token\")\n",
    "        print(\"\\nüîß To fix this:\")\n",
    "        print(\"1. Go to https://huggingface.co/settings/tokens\")\n",
    "        print(\"2. Create a new token with 'read' permissions\")\n",
    "        print(\"3. Add it to your .streamlit/secrets.toml file\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå UNEXPECTED ERROR: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OTHER ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nAccess Status: {'‚úÖ GRANTED' if access_granted else '‚ùå DENIED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf2e92",
   "metadata": {},
   "source": [
    "## Step 3: Implement Robust Error Handling\n",
    "\n",
    "Here's the improved version of your model loading function with proper error handling and fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_with_fallback(model_name: str, hf_token: str = None):\n",
    "    \"\"\"\n",
    "    Improved model setup function with proper error handling and fallbacks\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Setting up model: {model_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if model_name == \"Fine-tuned StarCoder\":\n",
    "        try:\n",
    "            print(\"üîÑ Attempting to load Fine-tuned StarCoder...\")\n",
    "            \n",
    "            # Load the adapter configuration first (lightweight test)\n",
    "            config = PeftConfig.from_pretrained(\n",
    "                \"ArneKreuz/starcoderbase-finetuned-thestack\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(\"‚úÖ Adapter config loaded successfully\")\n",
    "            \n",
    "            # Load the base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"bigcode/starcoderbase-3b\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(\"‚úÖ Base model loaded successfully\")\n",
    "            \n",
    "            # Load the fine-tuned model\n",
    "            model = PeftModel.from_pretrained(\n",
    "                base_model, \n",
    "                \"ArneKreuz/starcoderbase-finetuned-thestack\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(\"‚úÖ Fine-tuned StarCoder model loaded successfully!\")\n",
    "            return model\n",
    "            \n",
    "        except OSError as e:\n",
    "            if \"403\" in str(e) or \"gated\" in str(e):\n",
    "                print(\"‚ùå Access denied to gated repository\")\n",
    "                print(\"üîÑ Falling back to public Zephyr-7b model...\")\n",
    "            else:\n",
    "                print(f\"‚ùå OSError: {e}\")\n",
    "                print(\"üîÑ Falling back to public model...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            print(\"üîÑ Falling back to public model...\")\n",
    "    \n",
    "    # Fallback to public model (Zephyr-7b via API endpoint)\n",
    "    try:\n",
    "        print(\"üîÑ Loading Zephyr-7b via Hugging Face Inference API...\")\n",
    "        endpoint_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
    "        \n",
    "        model = HuggingFaceEndpoint(\n",
    "            endpoint_url=endpoint_url,\n",
    "            task=\"text-generation\",\n",
    "            max_new_tokens=512,\n",
    "            top_k=50,\n",
    "            temperature=0.1,\n",
    "            repetition_penalty=1.03,\n",
    "            huggingfacehub_api_token=hf_token\n",
    "        )\n",
    "        print(\"‚úÖ Zephyr-7b model loaded successfully as fallback!\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load fallback model: {e}\")\n",
    "        raise RuntimeError(\"Could not load any model - check your internet connection and token\")\n",
    "\n",
    "# Test the improved function\n",
    "try:\n",
    "    test_model = setup_model_with_fallback(\"Fine-tuned StarCoder\", hf_token)\n",
    "    print(f\"\\nüéâ Model setup completed successfully!\")\n",
    "    print(f\"Model type: {type(test_model).__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nüí• Model setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96e22a",
   "metadata": {},
   "source": [
    "## Step 4: Test Fallback Model Inference\n",
    "\n",
    "Let's test that the fallback model works correctly for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00acb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with the loaded model\n",
    "test_prompt = \"\"\"Generate G-code for a simple milling operation:\n",
    "- Material: Aluminum\n",
    "- Operation: Face milling\n",
    "- Workpiece size: 50mm x 50mm x 10mm\n",
    "- Tool: 10mm end mill\"\"\"\n",
    "\n",
    "print(\"üß™ Testing model inference...\")\n",
    "print(\"Prompt:\", test_prompt)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    if hasattr(test_model, 'invoke'):\n",
    "        # For HuggingFaceEndpoint models\n",
    "        response = test_model.invoke(test_prompt)\n",
    "    else:\n",
    "        # For transformers models (if we successfully loaded StarCoder)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderbase-3b\", token=hf_token)\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "        outputs = test_model.generate(**inputs, max_length=200, temperature=0.7)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"‚úÖ Model inference successful!\")\n",
    "    print(\"Response preview:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(response[:500] + \"...\" if len(response) > 500 else response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Inference failed: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. API rate limiting\")\n",
    "    print(\"2. Model loading issues\")\n",
    "    print(\"3. Network connectivity problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4c8a0",
   "metadata": {},
   "source": [
    "## Step 5: Apply Fix to Your Main Application\n",
    "\n",
    "Now that we've tested the authentication and fallback mechanism, let's apply this fix to your main application.\n",
    "\n",
    "### ‚úÖ Summary of Issues Fixed:\n",
    "\n",
    "1. **Added proper authentication** - Login with token before accessing models\n",
    "2. **Implemented error handling** - Catch OSError for gated repositories  \n",
    "3. **Added fallback mechanism** - Use public Zephyr-7b when StarCoder isn't accessible\n",
    "4. **Improved user messaging** - Clear instructions for requesting access\n",
    "\n",
    "### üîß Next Steps:\n",
    "\n",
    "1. **Update your `model_utils.py`** - The improved version has already been applied\n",
    "2. **Request access to gated models**: \n",
    "   - Visit https://huggingface.co/bigcode/starcoderbase-3b\n",
    "   - Click \"Request Access\" and fill out the form\n",
    "   - Wait for approval (usually 1-2 business days)\n",
    "3. **Verify your token has correct permissions**:\n",
    "   - Go to https://huggingface.co/settings/tokens\n",
    "   - Ensure token has \"Read repositories\" permission\n",
    "4. **Test your application** - It should now fallback gracefully to Zephyr-7b\n",
    "\n",
    "### üéØ Expected Behavior:\n",
    "\n",
    "- ‚úÖ If you have access: Uses Fine-tuned StarCoder as intended\n",
    "- ‚úÖ If access denied: Automatically falls back to Zephyr-7b with informative message\n",
    "- ‚úÖ No more crashes: Application continues working regardless of gated model access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ef365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification checklist\n",
    "print(\"üîç FINAL VERIFICATION CHECKLIST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check 1: Token availability\n",
    "token_status = \"‚úÖ Available\" if hf_token else \"‚ùå Missing\"\n",
    "print(f\"1. Hugging Face Token: {token_status}\")\n",
    "\n",
    "# Check 2: Authentication status  \n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami(token=hf_token) if hf_token else None\n",
    "    auth_status = f\"‚úÖ Authenticated as {user_info['name']}\" if user_info else \"‚ùå Not authenticated\"\n",
    "except:\n",
    "    auth_status = \"‚ùå Authentication failed\"\n",
    "print(f\"2. Authentication: {auth_status}\")\n",
    "\n",
    "# Check 3: Gated model access\n",
    "gated_access = \"‚úÖ Access granted\" if access_granted else \"‚ùå Access denied (using fallback)\"\n",
    "print(f\"3. StarCoder Access: {gated_access}\")\n",
    "\n",
    "# Check 4: Fallback model \n",
    "fallback_status = \"‚úÖ Working\" if 'test_model' in locals() else \"‚ùå Failed to load\"\n",
    "print(f\"4. Fallback Model: {fallback_status}\")\n",
    "\n",
    "print(\"\\nüéØ RECOMMENDED ACTIONS:\")\n",
    "if not hf_token:\n",
    "    print(\"‚Ä¢ Add your Hugging Face token to .streamlit/secrets.toml\")\n",
    "if not access_granted:\n",
    "    print(\"‚Ä¢ Request access to https://huggingface.co/bigcode/starcoderbase-3b\") \n",
    "    print(\"‚Ä¢ Your application will use Zephyr-7b until access is granted\")\n",
    "if auth_status.startswith(\"‚ùå\"):\n",
    "    print(\"‚Ä¢ Verify your token is valid and has correct permissions\")\n",
    "\n",
    "print(\"\\n‚úÖ Your application should now run without crashes!\")\n",
    "print(\"The fixes have been applied to your model_utils.py file.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
