{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/EUR/moabd/.cache/pypoetry/virtualenvs/gllm-Z7QQw8HW-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/EUR/moabd/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import toml\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "# Define the path to the secrets.toml file\n",
    "secrets_file_path = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..', '.streamlit', 'secrets.toml'))\n",
    "\n",
    "# Load the secrets\n",
    "secrets = toml.load(secrets_file_path)\n",
    "\n",
    "# Accessing the Hugging Face token and OpenAI token from the secrets\n",
    "hf_token = secrets[\"huggingface_token\"]\n",
    "openai_token = secrets[\"openai_token\"]\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/EUR/moabd/.cache/pypoetry/virtualenvs/gllm-Z7QQw8HW-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the base model\n",
    "config = PeftConfig.from_pretrained(\"ArneKreuz/starcoderbase-finetuned-thestack\", token=hf_token)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoderbase-3b\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderbase-3b\")\n",
    "\n",
    "# If the tokenizer does not have a padding token, set it to be the same as eos_token\n",
    "if tokenizer.pad_token is None: \n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine tuned model\n",
    "model = PeftModel.from_pretrained(base_model, \"ArneKreuz/starcoderbase-finetuned-thestack\", token=hf_token, force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Generate G-code with the following instructions: \n",
    "Machine: Siemens 840D CNC Mill\n",
    "Operation: Milling\n",
    "Material: Steel\n",
    "Workpiece Dimensions: 100mm x 100mm x 20mm\n",
    "Tool Selection:\n",
    "  - Milling Tool: Carbide end mill, 10mm diameter, 4-flute\n",
    "Toolpath Strategy: The sequence includes moving to the start position, cutting a square shape in the material, and then returning to the home position to end the program.\n",
    "Machining Parameters:\n",
    "  - Cutting Speed: 500 m/min\n",
    "  - Feed Rate: 200 mm/min\n",
    "  - Depth of Cut: 5 mm\n",
    "  - Stepover: 50%\n",
    "Safety Considerations:\n",
    "  - Rapid Retract Height: 10 mm\n",
    "  - Tool Clearance: 2 mm\n",
    "Your answer should have only the G-code without explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/EUR/moabd/.cache/pypoetry/virtualenvs/gllm-Z7QQw8HW-py3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Encode the instructions and generate the G-code\n",
    "input_ids = tokenizer.encode(instructions, return_tensors='pt')\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).int()\n",
    "output = base_model.generate(input_ids, attention_mask=attention_mask, max_length=2000, temperature=0.7)\n",
    "\n",
    "# Decode the output to get the G-code\n",
    "g_code = tokenizer.decode(output[0])\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(g_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gllm-Z7QQw8HW-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
